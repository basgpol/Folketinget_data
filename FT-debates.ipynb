{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcripts of parliamentary debates in Denmark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook consist of three parts:\n",
    "    1. Part 1 loads all the required packages\n",
    "    2. Part 2 creates a function that collects url to transscripts of danish parliamentary debates\n",
    "    3. Part 3 creates a function that collects the title, date and content of a parliamentary debate from a url \n",
    "    4. Collects the data and save it as a CSV-file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from datascience import *\n",
    "import re\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: URL-collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ft_url_collect(year = None, month = None, day = None):\n",
    "    \"\"\" Function that collects url-links to transcript of danish parliamentary debates\n",
    "        The function takes three imports\n",
    "        \n",
    "        1) The year of the start date you want (e.g. \"2012\"). The default is year \"2000\".\n",
    "        2) The month written as two-digits of the start date you want (e.g. \"07\"). The default is \"01\" (january).\n",
    "        3) The day written as two-digits of the start date you want (e.g. \"31\"). The default is is \"01\"\n",
    "        \n",
    "        By 14th March 2018 the transcripts go back to 5th October 2004 \n",
    "        \"\"\"\n",
    "    base_url = \"http://www.ft.dk/da/dokumenter/dokumentlister/referater?pageSize=200&startDate=\"\n",
    "    \n",
    "    if year == None:\n",
    "        year = \"2000\"\n",
    "    if month == None:\n",
    "        month = \"01\"\n",
    "    if day == None:\n",
    "        day = \"01\"\n",
    "    \n",
    "    startdate = str(year)+str(month)+str(day) #creating start date\n",
    "    url = base_url+startdate #creating url with links to debate transcripts\n",
    "    \n",
    "    response = requests.get(url) # GET-request\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    np_links = np.array(\"link\") # creating empty numpy array\n",
    "\n",
    "    # Creating a loop that collects every link and only keep the links that contain \"forhandling\" in th url\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        every_link = link.get(\"href\")\n",
    "        if every_link[1:14] == \"forhandlinger\": \n",
    "            np_links = np.append(np_links, \"http://www.ft.dk\"+every_link)\n",
    "\n",
    "    links = np_links[1:] # drop the first irrelevant element\n",
    "    \n",
    "    links = np.unique(links) # drop duplicates\n",
    "    \n",
    "    return (links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['http://www.ft.dk/forhandlinger/20151/20151M096_2016-05-18_1300.htm',\n",
       "       'http://www.ft.dk/forhandlinger/20151/20151M097_2016-05-19_1000.htm',\n",
       "       'http://www.ft.dk/forhandlinger/20151/20151M098_2016-05-20_1000.htm',\n",
       "       'http://www.ft.dk/forhandlinger/20151/20151M099_2016-05-23_1000.htm',\n",
       "       'http://www.ft.dk/forhandlinger/20151/20151M100_2016-05-24_1300.htm'],\n",
       "      dtype='<U66')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the collector\n",
    "ft_url_collect(year = \"1999\", month = \"07\", day = \"01\")[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Transcript collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_title_date_text(debate_url):\n",
    "    \"\"\"This function takes as input an URL with the transscript of the parliamentary debate in html-format \n",
    "       and return a np.array with three elements: title of the debate, date of the debate, and a string \n",
    "       with the content of the debate\"\"\"\n",
    "    \n",
    "    response = requests.get(debate_url) # GET-request\n",
    "    soup = BeautifulSoup(response.content, 'html.parser') #turn into a soup\n",
    "    \n",
    "    # Finding element 1: The title of the debate\n",
    "    title = soup.find(\"p\", attrs={'class':'Titel'}).text\n",
    "    \n",
    "    # Finding element 2: The date and time of the debate\n",
    "    date = soup.find(\"meta\", attrs={'name':'DateOfSitting'}).get(\"content\")\n",
    "    \n",
    "    # Finding element 3: The content of the debate (Everything that was said in the debate)\n",
    "    all_text_parts = soup.find_all(\"p\", attrs={'class':'Tekst'}) + soup.find_all(\"p\", attrs={'class':'TekstIndryk'}) #getting a list with all text parts\n",
    "    all_text = \"\" #creating a empty character string\n",
    "    \n",
    "    for text_part in all_text_parts: #creating a loop that take all text parts and collects them in one string\n",
    "        text = text_part.text\n",
    "        all_text = all_text + text + \" \"\n",
    "    all_text = all_text.replace(\"\\n\", \"\") #removing \\n\n",
    "    \n",
    "    # Collecting all elements in one np.array\n",
    "    result = [title, date, all_text] \n",
    "    \n",
    "    return(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18. mÃ¸de', '2017-11-14T13:00:00']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the transcript collector function\n",
    "test_url = \"http://www.ft.dk/forhandlinger/20171/20171M018_2017-11-14_1300.htm\"\n",
    "scrape_title_date_text(test_url)[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 4: Scraping transcripts for debates since 1-1-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting URL's with debate transcripts in html-format since 1-1-2017\n",
    "ft_urls = ft_url_collect(year = \"2017\", month = \"01\", day = \"01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['http://www.ft.dk/forhandlinger/20161/20161M041_2017-01-11_1300.htm',\n",
       "       'http://www.ft.dk/forhandlinger/20161/20161M042_2017-01-12_1000.htm',\n",
       "       'http://www.ft.dk/forhandlinger/20161/20161M043_2017-01-13_1000.htm',\n",
       "       'http://www.ft.dk/forhandlinger/20161/20161M044_2017-01-17_1300.htm',\n",
       "       'http://www.ft.dk/forhandlinger/20161/20161M045_2017-01-18_1300.htm',\n",
       "       'http://www.ft.dk/forhandlinger/20161/20161M046_2017-01-19_1000.htm',\n",
       "       'http://www.ft.dk/forhandlinger/20161/20161M047_2017-01-20_1000.htm',\n",
       "       'http://www.ft.dk/forhandlinger/20161/20161M048_2017-01-24_1400.htm',\n",
       "       'http://www.ft.dk/forhandlinger/20161/20161M049_2017-01-25_1300.htm'],\n",
       "      dtype='<U66')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the result\n",
    "ft_urls[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\datascience\\tables.py:81: FutureWarning: Table.empty(labels) is deprecated. Use Table(labels)\n",
      "  warnings.warn(\"Table.empty(labels) is deprecated. Use Table(labels)\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\datascience\\tables.py:49: FutureWarning: Two-argument __init__ is deprecated. Use Table().with_columns(...)\n",
      "  warnings.warn(\"Two-argument __init__ is deprecated. Use Table().with_columns(...)\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Title</th> <th>Date</th> <th>Text</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Title | Date | Text"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a empty table\n",
    "t = Table().empty(make_array(\"Title\", \"Date\", \"Text\"))\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, I use the scraper function to collect transcripts of parliamentary debates in Denmark. I cap the loop at 50 iteration, because the webside of the danish parliament cut us off after 50 url-calls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a loop that scrape debate data from each url\n",
    "debates_list = [] #creating a empty list\n",
    "\n",
    "for url in ft_urls[0:50]:\n",
    "   debate_data = scrape_title_date_text(url) #scraping data from url using scraper-function\n",
    "   debates_list.append(debate_data) #appending scraped data to list\n",
    "   time.sleep(2)  # waits 2 seconds before next iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Title</th> <th>Date</th> <th>Text</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>40. mÃ¸de</td> <td>2017-01-10T13:00:00</td> <td>MÃ¸det er Ã¥bnet. 1) 3. behandling af lovforslag nr. L 92  ...</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>41. mÃ¸de</td> <td>2017-01-11T13:00:00</td> <td>MÃ¸det er Ã¥bnet. 1) Besvarelse af oversendte spÃ¸rgsmÃ¥l ti ...</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>42. mÃ¸de</td> <td>2017-01-12T10:00:00</td> <td>MÃ¸det er Ã¥bnet. 1) SpÃ¸rgsmÃ¥l om fremme af forespÃ¸rgsel n ...</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>43. mÃ¸de</td> <td>2017-01-13T10:00:00</td> <td>MÃ¸det er Ã¥bnet. 1) 1. behandling af lovforslag nr. L 100 ...</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>44. mÃ¸de</td> <td>2017-01-17T13:00:00</td> <td>MÃ¸det er Ã¥bnet. 1) SpÃ¸rgetime med statsministeren. Jeg g ...</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>45. mÃ¸de</td> <td>2017-01-18T13:00:00</td> <td>MÃ¸det er Ã¥bnet. 1) Besvarelse af oversendte spÃ¸rgsmÃ¥l ti ...</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>46. mÃ¸de</td> <td>2017-01-19T10:00:00</td> <td>MÃ¸det er Ã¥bnet. 1) SpÃ¸rgsmÃ¥l om fremme af forespÃ¸rgsel n ...</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>47. mÃ¸de</td> <td>2017-01-20T10:00:00</td> <td>MÃ¸det er Ã¥bnet. 1) SpÃ¸rgsmÃ¥l om fremme af forespÃ¸rgsel n ...</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>48. mÃ¸de</td> <td>2017-01-24T14:00:00</td> <td>MÃ¸det er Ã¥bnet. 1) SpÃ¸rgsmÃ¥l om fremme af forespÃ¸rgsel n ...</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>49. mÃ¸de</td> <td>2017-01-25T13:00:00</td> <td>MÃ¸det er Ã¥bnet. 1) Besvarelse af oversendte spÃ¸rgsmÃ¥l ti ...</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (39 rows omitted)</p"
      ],
      "text/plain": [
       "Title    | Date                | Text\n",
       "40. mÃ¸de | 2017-01-10T13:00:00 | MÃ¸det er Ã¥bnet. 1) 3. behandling af lovforslag nr. L 92  ...\n",
       "41. mÃ¸de | 2017-01-11T13:00:00 | MÃ¸det er Ã¥bnet. 1) Besvarelse af oversendte spÃ¸rgsmÃ¥l ti ...\n",
       "42. mÃ¸de | 2017-01-12T10:00:00 | MÃ¸det er Ã¥bnet. 1) SpÃ¸rgsmÃ¥l om fremme af forespÃ¸rgsel n ...\n",
       "43. mÃ¸de | 2017-01-13T10:00:00 | MÃ¸det er Ã¥bnet. 1) 1. behandling af lovforslag nr. L 100 ...\n",
       "44. mÃ¸de | 2017-01-17T13:00:00 | MÃ¸det er Ã¥bnet. 1) SpÃ¸rgetime med statsministeren. Jeg g ...\n",
       "45. mÃ¸de | 2017-01-18T13:00:00 | MÃ¸det er Ã¥bnet. 1) Besvarelse af oversendte spÃ¸rgsmÃ¥l ti ...\n",
       "46. mÃ¸de | 2017-01-19T10:00:00 | MÃ¸det er Ã¥bnet. 1) SpÃ¸rgsmÃ¥l om fremme af forespÃ¸rgsel n ...\n",
       "47. mÃ¸de | 2017-01-20T10:00:00 | MÃ¸det er Ã¥bnet. 1) SpÃ¸rgsmÃ¥l om fremme af forespÃ¸rgsel n ...\n",
       "48. mÃ¸de | 2017-01-24T14:00:00 | MÃ¸det er Ã¥bnet. 1) SpÃ¸rgsmÃ¥l om fremme af forespÃ¸rgsel n ...\n",
       "49. mÃ¸de | 2017-01-25T13:00:00 | MÃ¸det er Ã¥bnet. 1) Besvarelse af oversendte spÃ¸rgsmÃ¥l ti ...\n",
       "... (39 rows omitted)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debate_table = t.with_rows(debates_list)\n",
    "debate_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exporting data as a CSV\n",
    "debate_table.to_df().to_csv(\"debate_text_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
